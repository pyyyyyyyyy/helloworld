{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "tensor([[ 0.2113, -0.1902,  0.0155, -0.0361,  0.3048, -0.0380, -0.0743, -0.2366,\n",
      "          0.0536, -0.0602],\n",
      "        [ 0.1542, -0.1388, -0.1212, -0.0085,  0.1517, -0.1535, -0.1191, -0.2029,\n",
      "          0.0537, -0.1713],\n",
      "        [ 0.1749, -0.1326, -0.0131, -0.2024,  0.2266, -0.0469, -0.0444, -0.2532,\n",
      "          0.0509, -0.0664],\n",
      "        [ 0.1858, -0.1601,  0.0137, -0.0280,  0.2012, -0.0778, -0.0259, -0.2144,\n",
      "          0.1152, -0.1440],\n",
      "        [ 0.1893, -0.1607,  0.0232, -0.0368,  0.2341, -0.0705,  0.0115, -0.1640,\n",
      "          0.0111, -0.0551]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "### 构造网络结构\n",
    "#1 通过直接继承Module构造：\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        a = self.act(self.hidden(x))\n",
    "        return self.output(a)\n",
    "\n",
    "X = torch.rand(5,784)\n",
    "net = MLP()\n",
    "print(net)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "tensor([[-0.0062, -0.2031, -0.2185, -0.0021,  0.1420, -0.0909,  0.0839, -0.0424,\n",
      "         -0.1877, -0.1347],\n",
      "        [-0.1186, -0.1491, -0.0931, -0.1109,  0.2663, -0.1916,  0.2269, -0.1162,\n",
      "         -0.1705, -0.1301],\n",
      "        [-0.0909, -0.1485, -0.0136, -0.0706,  0.1453, -0.1295,  0.0940,  0.0174,\n",
      "         -0.1262, -0.0808],\n",
      "        [ 0.0209, -0.1949, -0.1067,  0.0553,  0.1143, -0.0746,  0.0643, -0.1399,\n",
      "          0.0336, -0.0140],\n",
      "        [-0.0570, -0.1424, -0.1451, -0.1573,  0.0951, -0.1820,  0.0922, -0.0609,\n",
      "         -0.1194, -0.1428]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "### 构造网络结构\n",
    "#2 通过继承Module子类Sequential构造：\n",
    "\n",
    "net = nn.Sequential(nn.Linear(784, 256) ,nn.ReLU(), nn.Linear(256, 10))\n",
    "print(net)\n",
    "X = torch.rand(5,784)\n",
    "print(net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleDict(\n",
      "  (act): ReLU()\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "ReLU()\n",
      "Linear(in_features=256, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "### 构造网络结构\n",
    "#3 通过继承Module子类ModuleDict构造：\n",
    "# 作为字典操作\n",
    "\n",
    "net = nn.ModuleDict({'hidden':nn.Linear(784, 256),\n",
    "                    'act':nn.ReLU(),\n",
    "                    })\n",
    "net['output'] = nn.Linear(256, 10)\n",
    "print(net)\n",
    "print(net['act'])\n",
    "print(net.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Linear(in_features=256, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "### 构造网络结构\n",
    "#4 通过继承Module子类ModuleList构造：\n",
    "# 作为列表，可使用append、extend等操作\n",
    "\n",
    "net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])\n",
    "net.append(nn.Linear(256, 10))\n",
    "print(net)\n",
    "print(net[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))\n",
    "# pytorch已默认进行初始化\n",
    "\n",
    "print(net)\n",
    "X = torch.rand(2, 4)\n",
    "Y = net(X).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6344, 0.9445, 0.7919, 0.0786],\n",
      "        [0.1679, 0.9494, 0.2991, 0.1006]]) tensor(0.6875, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "0.weight torch.Size([3, 4])\n",
      "0.bias torch.Size([3])\n",
      "2.weight torch.Size([1, 3])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# 通过继承Module类中paramters()或named_paramters()方法，来访问网络中所有参数（以迭代器形式返回）。后者除了返回参数tensor还会返回名字\n",
    "print(type(net.named_parameters()))\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())\n",
    "\n",
    "# param是一个tensor，data(),size(),grad()\n",
    "# 可见返回的名字前自动带了层数前缀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "# 访问单层参数\n",
    "for name, param in net[0].named_parameters():\n",
    "    print(name, param.size(), type(param))\n",
    "\n",
    "# 因为是单层访问，所以名字前方没有数字前缀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel()\n",
      "weight1\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyModel, self).__init__(**kwargs)\n",
    "        self.weight1 = nn.Parameter(torch.rand(20, 20))\n",
    "        self.weight2 = torch.rand(20, 20)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        pass\n",
    "\n",
    "net0 = MyModel()\n",
    "print(net0)\n",
    "for name, param in net0.named_parameters():\n",
    "    print(name)\n",
    "    \n",
    "# weight1使用了nn.Parameter(),在参数列表中\n",
    "# weight2不在参数列表中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4269, -0.2976, -0.4891, -0.3140],\n",
      "        [-0.0497,  0.4347, -0.2373, -0.1218],\n",
      "        [ 0.4373, -0.2428, -0.3731, -0.1160]])\n",
      "None\n",
      "tensor([[0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2486, 0.5868, 0.3381, 0.0556],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# Parameter本质是tensor,tensor有的属性它都有，例如可以通过data()访问参数数值，通过grad()访问参数梯度\n",
    "weight_0 = list(net[0].parameters())[0]\n",
    "print(weight_0.data)\n",
    "print(weight_0.grad)\n",
    "# 反向传播前梯度为None\n",
    "\n",
    "Y.backward()\n",
    "print(weight_0.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-0.0045,  0.0029,  0.0183,  0.0001],\n",
      "        [ 0.0018, -0.0155, -0.0085,  0.0012],\n",
      "        [-0.0115,  0.0060, -0.0210,  0.0093]])\n",
      "2.weight tensor([[ 0.0022,  0.0082, -0.0178]])\n"
     ]
    }
   ],
   "source": [
    "### 初始化模型参数\n",
    "# 使用pytorch的init中自带的多种预设的初始化方法来初始化权重（torch.nn.init.normal_())\n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        # 对权重进行随机正态分布\n",
    "        init.normal_(param, mean=0, std=0.01)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([0., 0., 0.])\n",
      "2.bias tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# 使用常数进行初始化(torch.nn.init.constant_())\n",
    "for name, param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        # 对偏置进行常数初始化\n",
    "        init.constant_(param, val=0)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight Parameter containing:\n",
      "tensor([[-0.0045,  0.0029,  0.0183,  0.0001],\n",
      "        [ 0.0018, -0.0155, -0.0085,  0.0012],\n",
      "        [-0.0115,  0.0060, -0.0210,  0.0093]], requires_grad=True)\n",
      "0.bias Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n",
      "2.weight Parameter containing:\n",
      "tensor([[ 0.0022,  0.0082, -0.0178]], requires_grad=True)\n",
      "2.bias Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[ 5.5880, -7.0385,  0.0000, -6.9496],\n",
      "        [ 0.0000, -7.9420,  6.6985,  6.5157],\n",
      "        [-6.0789, -0.0000,  0.0000, -0.0000]])\n",
      "2.weight tensor([[ 0.0000, -9.4091, -9.9957]])\n"
     ]
    }
   ],
   "source": [
    "# 使用自定义的方法初始化权重。\n",
    "# 参考init中预定义的初始化方法，可以看到就是一个改变tensor数值的函数，且这个过程中不记录梯度。\n",
    "\n",
    "def init_weight_(tensor):\n",
    "    # 初始化过程中不记录梯度\n",
    "    with torch.no_grad():\n",
    "        # uniform_(x, y)表示在[x, y]中随机抽样\n",
    "        tensor.uniform_(-10, 10)\n",
    "        # 表示有一半的概率为0，另一半的概率初始化为[-10，-5],[5, 10]之间均匀分布的随机数\n",
    "        tensor *= (tensor.abs() >= 5).float()\n",
    "        \n",
    "for name, param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init_weight_(param)\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([1., 1., 1.])\n",
      "2.bias tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# 改变bias值（param.data），同时不影响梯度\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        param.data += 1\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=3, out_features=3, bias=False)\n",
      "  (1): Linear(in_features=3, out_features=3, bias=False)\n",
      ")\n",
      "0.weight torch.Size([3, 3]) tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 共享模型参数\n",
    "# 如果传入Sequential的是同一个Module实例，则他们的参数共享\n",
    "linear = nn.Linear(3, 3, bias=False)\n",
    "net = nn.Sequential(linear, linear)\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    init.constant_(param, val=2)\n",
    "    print(name, param.size(), param.data)\n",
    "    \n",
    "\n",
    "# 网络的两层对应同一个对象\n",
    "print(id(net[0]) == id(net[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]]) tensor(540., grad_fn=<SumBackward0>)\n",
      "tensor([[60., 60., 60.],\n",
      "        [60., 60., 60.],\n",
      "        [60., 60., 60.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.ones(5, 3)\n",
    "y = net(X).sum()\n",
    "print(X,y)\n",
    "\n",
    "# 因为是同一个对象，所以共享的参数梯度是累加的\n",
    "y.backward()\n",
    "print(net[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2., -1.,  0.,  1.,  2.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自定义层\n",
    "# 定义一个不含模型参数的自定义层\n",
    "\n",
    "class CentreLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CentreLayer,self).__init__(**kwargs)\n",
    "    def forward(self, x):\n",
    "        return x - x.mean()\n",
    "    \n",
    "layer = CentreLayer()\n",
    "layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.3551048040390015e-09"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential(nn.Linear(8, 128), CentreLayer())\n",
    "\n",
    "y = net(torch.rand(4, 8))\n",
    "\n",
    "y.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyListDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义一个含模型参数的自定义层（参数应该定义成nn.Parameter类型，可以自动被识别成模型参数）\n",
    "# 参数可使用ParameterList.(append,extend)\n",
    "\n",
    "class MyListDense(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyListDense, self).__init__(**kwargs)\n",
    "        # ParameterList是一个列表，每个元素均为nn.Parameter类型\n",
    "        self.params = nn.ParameterList(\n",
    "            [nn.Parameter(torch.randn(4,4)) for i in range(3)]\n",
    "        )\n",
    "        self.params.append(nn.Parameter(torch.ones(4,4)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x, self.params[i])\n",
    "        return x\n",
    "\n",
    "net = MyListDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (weight0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (weight1): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "      (weight2): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n",
      "tensor([[0.3357, 2.3771, 2.1859, 0.7698]], grad_fn=<MmBackward>)\n",
      "tensor([[-3.5406]], grad_fn=<MmBackward>)\n",
      "tensor([[-0.6335,  0.3732]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 定义一个含模型参数的自定义层（参数应该定义成nn.Parameter类型，可以自动被识别成模型参数）\n",
    "# 参数可使用ParameterDict.(update,keys)\n",
    "\n",
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyDictDense, self).__init__(**kwargs)\n",
    "        self.params = nn.ParameterDict({\n",
    "            'weight0':nn.Parameter(torch.randn(4, 4)),\n",
    "            'weight1':nn.Parameter(torch.randn(4, 1))                    \n",
    "        })\n",
    "        self.params.update({'weight2':nn.Parameter(torch.randn(4, 2))})\n",
    "        \n",
    "    def forward(self, x, choice= 'weight0'):\n",
    "        return torch.mm(x, self.params[choice])\n",
    "    \n",
    "net = MyDictDense()\n",
    "print(net)\n",
    "\n",
    "x = torch.ones(1,4)\n",
    "print(net(x, 'weight0'))\n",
    "print(net(x, 'weight1'))\n",
    "print(net(x, 'weight2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): MyListDense(\n",
      "    (params): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (3): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "    )\n",
      "  )\n",
      "  (1): MyDenseDict(\n",
      "    (params): ParameterDict(\n",
      "        (weight0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "        (weight1): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "        (weight2): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[-4.8454]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(MyListDense(), MyDenseDict())\n",
    "print(net)\n",
    "print(net(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取和存储\n",
    "# 使用save和load读取和保存tensor\n",
    "\n",
    "x = torch.ones(3)\n",
    "torch.save(x, 'x.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('x.pt')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1., 1., 1.]), tensor([0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存一个tensor列表\n",
    "\n",
    "y = torch.zeros(4)\n",
    "torch.save([x,y],'xy_list.pt')\n",
    "xy = torch.load('xy_list.pt')\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([1., 1., 1.]), 'y': tensor([0., 0., 0., 0.])}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存一个tensor字典\n",
    "\n",
    "torch.save({'x':x,'y':y},'xy_dict.pt')\n",
    "xy = torch.load('xy_dict.pt')\n",
    "xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[ 0.3969,  0.3525, -0.1302],\n",
       "                      [-0.5180, -0.4990, -0.3561],\n",
       "                      [ 0.0611,  0.4282,  0.1527],\n",
       "                      [ 0.3883,  0.3240, -0.0924],\n",
       "                      [-0.2087, -0.4826, -0.2822]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([-0.0319, -0.4559,  0.2143,  0.4169, -0.5517])),\n",
       "             ('output.weight',\n",
       "              tensor([[ 0.1568, -0.0559, -0.2843, -0.2305,  0.3241],\n",
       "                      [-0.2311, -0.1352, -0.0693,  0.0449,  0.4031]])),\n",
       "             ('output.bias', tensor([ 0.2086, -0.3661]))])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读写模型\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(3,5)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(5,2)\n",
    "        \n",
    "    def forward(self ,x):\n",
    "        return output(act(x))\n",
    "    \n",
    "net = MLP()\n",
    "net.state_dict()\n",
    "\n",
    "# state_dict()返回一个从参数名称映射到参数tensor的字典，只有可学习的对象才会出现在字典中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state': {},\n",
       " 'param_groups': [{'lr': 0.001,\n",
       "   'momentum': 0.9,\n",
       "   'dampening': 0,\n",
       "   'weight_decay': 0,\n",
       "   'nesterov': False,\n",
       "   'params': [0, 1, 2, 3]}]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(),lr=0.001, momentum=0.9)\n",
    "optimizer.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存和加载模型\n",
    "\n",
    "# 保存\n",
    "# torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# 加载\n",
    "# model = themodelclass(*args, **kwargs)\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU计算\n",
    "\n",
    "# .cuda()可以将CPU上的tensor转换到GPU上\n",
    "\n",
    "# 指定设备\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
