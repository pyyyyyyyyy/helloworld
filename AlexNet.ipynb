{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LENet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): Sigmoid()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "training on  cuda\n",
      "epoch 1, loss 1.7855, train acc 0.351, test acc 0.596, time 11.8 sec\n",
      "epoch 2, loss 0.4620, train acc 0.647, test acc 0.686, time 7.0 sec\n",
      "epoch 3, loss 0.2480, train acc 0.726, test acc 0.730, time 7.3 sec\n",
      "epoch 4, loss 0.1655, train acc 0.747, test acc 0.747, time 7.3 sec\n",
      "epoch 5, loss 0.1224, train acc 0.762, test acc 0.762, time 7.5 sec\n",
      "epoch 6, loss 0.0962, train acc 0.773, test acc 0.773, time 7.1 sec\n",
      "epoch 7, loss 0.0786, train acc 0.785, test acc 0.786, time 7.9 sec\n",
      "epoch 8, loss 0.0659, train acc 0.794, test acc 0.784, time 7.3 sec\n",
      "epoch 9, loss 0.0561, train acc 0.804, test acc 0.800, time 7.5 sec\n",
      "epoch 10, loss 0.0484, train acc 0.814, test acc 0.808, time 7.6 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "import math\n",
    "from LeNet import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([1, 256, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 96, 11, 4),#(input_channel, output_channel, kernal_size, stride, padding)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2),#(kernal_size, stride)\n",
    "            nn.Conv2d(96, 256, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2),\n",
    "            nn.Conv2d(256, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*5*5, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "#        print(feature.size())\n",
    "        return self.fc(feature.view(img.shape[0], -1))\n",
    "    \n",
    "\n",
    "net = AlexNet()\n",
    "print(net)\n",
    "\n",
    "#print(net.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernal_size: 11, stride: 4, padding: 0\n",
      "kernal_size: 3, stride: 2, padding: 0\n",
      "kernal_size: 5, stride: 1, padding: 2\n",
      "kernal_size: 3, stride: 2, padding: 0\n",
      "kernal_size: 3, stride: 1, padding: 1\n",
      "kernal_size: 3, stride: 1, padding: 1\n",
      "kernal_size: 3, stride: 1, padding: 1\n",
      "kernal_size: 3, stride: 2, padding: 0\n",
      "[[11, 4, 0], [3, 2, 0], [5, 1, 2], [3, 2, 0], [3, 1, 1], [3, 1, 1], [3, 1, 1], [3, 2, 0]]\n",
      "5 5\n"
     ]
    }
   ],
   "source": [
    "def compute_map_size(size):\n",
    "    param = []\n",
    "    for layer in net.modules():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            param.append([layer.kernel_size[0],layer.stride[0],layer.padding[0]])\n",
    "            print(\"kernal_size: %d, stride: %d, padding: %d\" % (layer.kernel_size[0],layer.stride[0],layer.padding[0]))\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            param.append([layer.kernel_size,layer.stride,layer.padding])\n",
    "            print(\"kernal_size: %d, stride: %d, padding: %d\" % (layer.kernel_size,layer.stride,layer.padding))\n",
    "    print(param)\n",
    "    cur_size = size\n",
    "    for k, s, p in param:\n",
    "        cur_size = (cur_size - k + 2*p + s) // s\n",
    "        size = math.floor((size + 2*p -k) / s) + 1\n",
    "    print(cur_size, size)\n",
    "compute_map_size(224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_fashion_mnist(batch_size, resize=None, root='D:\\Repos\\PycharmProjects\\Datasets\\FashionMNIST'):\n",
    "    # torchvision.transform包含一些图像变换方法\n",
    "    # 图像预处理\n",
    "    trans=[]\n",
    "    # 使用torchvision.transform.Resize(size)来实现图像大小变化\n",
    "    if resize:\n",
    "        trans.append(torchvision.transforms.Resize(size=resize))\n",
    "    # 图像格式转换成tensor\n",
    "    trans.append(torchvision.transforms.ToTensor()) \n",
    "    # 将所有transform操作合并\n",
    "    transform = torchvision.transforms.Compose(trans)\n",
    "    \n",
    "    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n",
    "    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True,transform=transform)\n",
    "    \n",
    "    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True)\n",
    "    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_iter, test_iter = load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "epoch 1, loss 0.4096, train acc 0.848, test acc 0.841, time 7.5 sec\n",
      "epoch 2, loss 0.2001, train acc 0.853, test acc 0.840, time 7.3 sec\n",
      "epoch 3, loss 0.1307, train acc 0.856, test acc 0.844, time 7.2 sec\n",
      "epoch 4, loss 0.0962, train acc 0.858, test acc 0.844, time 7.1 sec\n",
      "epoch 5, loss 0.0757, train acc 0.861, test acc 0.842, time 7.1 sec\n",
      "epoch 6, loss 0.0616, train acc 0.866, test acc 0.850, time 7.3 sec\n",
      "epoch 7, loss 0.0522, train acc 0.867, test acc 0.851, time 7.0 sec\n",
      "epoch 8, loss 0.0447, train acc 0.869, test acc 0.856, time 7.1 sec\n",
      "epoch 9, loss 0.0393, train acc 0.870, test acc 0.857, time 7.3 sec\n",
      "epoch 10, loss 0.0348, train acc 0.872, test acc 0.859, time 7.1 sec\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train(net, train_iter, test_iter, optimizer, loss, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
